{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAG Example (PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from utils import Logger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from gtrick import FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout, conv_type):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if conv_type == 'gcn':\n",
    "                if i == 0:\n",
    "                    self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "                elif i == num_layers - 1:\n",
    "                    self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "                else:\n",
    "                    self.convs.append(\n",
    "                        GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            elif conv_type == 'sage':\n",
    "                if i == 0:\n",
    "                    self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "                elif i == num_layers - 1:\n",
    "                    self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "                else:\n",
    "                    self.convs.append(\n",
    "                        SAGEConv(hidden_channels, hidden_channels))\n",
    "            \n",
    "            if i != num_layers - 1:\n",
    "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    # add a param perturb to pass perturb\n",
    "    def forward(self, x, adj_t, perturb=None):\n",
    "        # add perturb to x, note that do not use x += perturb\n",
    "        if perturb is not None:\n",
    "            x = x + perturb\n",
    "\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass flag to train func\n",
    "def train(model, data, train_idx, flag):\n",
    "    y = data.y[train_idx]\n",
    "\n",
    "    # define a forward func to get the output of the model\n",
    "    forward = lambda perturb: model(data.x, data.adj_t, perturb)[train_idx]\n",
    "\n",
    "    # run flag to get loss and output\n",
    "    loss, out = flag(model, forward, data.x.shape[0], y.squeeze(1))\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, eval_metric):\n",
    "    model.eval()\n",
    "\n",
    "    y = data.y\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })[eval_metric]\n",
    "    valid_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })[eval_metric]\n",
    "    test_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })[eval_metric]\n",
    "\n",
    "    return train_metric, valid_metric, test_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_node_pred(args, model, dataset):\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    evaluator = Evaluator(name=args.dataset)\n",
    "\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']\n",
    "\n",
    "    logger = Logger(args.runs, mode='max')\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=args.patience, verbose=True, mode='max')\n",
    "\n",
    "        if dataset.task_type == 'binary classification':\n",
    "            loss_func = nn.BCEWithLogitsLoss()\n",
    "        elif dataset.task_type == 'multiclass classification':\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # define flag, params: in_feats, loss_func, optimizer\n",
    "        flag = FLAG(data.x.shape[1], loss_func, optimizer)\n",
    "\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, flag)\n",
    "            result = test(model, data, split_idx,\n",
    "                          evaluator, dataset.eval_metric)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            train_acc, valid_acc, test_acc = result\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "            if early_stopping(valid_acc, model):\n",
    "                break\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='train node property prediction')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"ogbn-arxiv\",\n",
    "                    choices=[\"ogbn-arxiv\"])\n",
    "parser.add_argument(\"--dataset_path\", type=str, default=\"/dev/dataset\",\n",
    "                    help=\"path to dataset\")\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--log_steps', type=int, default=1)\n",
    "parser.add_argument('--model', type=str, default='sage')\n",
    "parser.add_argument('--num_layers', type=int, default=3)\n",
    "parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--epochs', type=int, default=500)\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--patience', type=int, default=10)\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(\n",
    "    name=args.dataset, \n",
    "    transform=T.ToSparseTensor(), \n",
    "    root=args.dataset_path\n",
    "    )\n",
    "data = dataset[0]\n",
    "\n",
    "num_features = data.x.shape[1]\n",
    "\n",
    "model = GNN(num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node_pred(args, model, dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
