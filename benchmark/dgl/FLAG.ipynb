{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator, collate_dgl\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dgl.nn import AvgPooling\n",
    "\n",
    "from utils import Logger, EarlyStopping\n",
    "from model import EGCNConv, EGINConv\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from gtrick import FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "\n",
    "        super(EGCN, self).__init__()\n",
    "\n",
    "        self.node_encoder = AtomEncoder(hidden_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                EGCNConv(hidden_channels))\n",
    "            if i != num_layers - 1:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pool = AvgPooling()\n",
    "\n",
    "        self.out = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for emb in self.node_encoder.atom_embedding_list:\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "\n",
    "        num_layers = len(self.convs)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs[i].reset_parameters()\n",
    "            if i != num_layers - 1:\n",
    "                self.bns[i].reset_parameters()\n",
    "\n",
    "        self.out.reset_parameters()\n",
    "\n",
    "    def forward(self, g, x, ex, perturb=None):\n",
    "        h = self.node_encoder(x)\n",
    "\n",
    "        if perturb is not None:\n",
    "            h += perturb\n",
    "\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            h = conv(g, h, ex)\n",
    "            h = self.bns[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.convs[-1](g, h, ex)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "\n",
    "        h = self.pool(g, h)\n",
    "        h = self.out(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGIN(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "\n",
    "        super(EGIN, self).__init__()\n",
    "\n",
    "        self.node_encoder = AtomEncoder(hidden_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                EGINConv(hidden_channels))\n",
    "            if i != num_layers - 1:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pool = AvgPooling()\n",
    "\n",
    "        self.out = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for emb in self.node_encoder.atom_embedding_list:\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "            \n",
    "        num_layers = len(self.convs)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs[i].reset_parameters()\n",
    "            if i != num_layers - 1:\n",
    "                self.bns[i].reset_parameters()\n",
    "\n",
    "        self.out.reset_parameters()\n",
    "\n",
    "    def forward(self, g, x, ex, perturb=None):\n",
    "        h = self.node_encoder(x)\n",
    "\n",
    "        if perturb is not None:\n",
    "            h += perturb\n",
    "\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            h = conv(g, h, ex)\n",
    "            h = self.bns[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.convs[-1](g, h, ex)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "\n",
    "        h = self.pool(g, h)\n",
    "        h = self.out(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, loader, flag):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc='Train'):\n",
    "        g, y = batch\n",
    "        g, y = g.to(device), y.to(device)\n",
    "\n",
    "        forward = lambda perturb: model(g, g.ndata['feat'], g.edata['feat'], perturb)\n",
    "\n",
    "        loss, out = flag(model, forward, g.num_nodes(), y.float())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return total_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, device, loader, evaluator, eval_metric):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in tqdm(loader, desc='Eval '):\n",
    "        g, y = batch\n",
    "        g, y = g.to(device), y.to(device)\n",
    "\n",
    "        yh = model(g, g.ndata['feat'], g.edata['feat'])\n",
    "\n",
    "        y_true.append(y.view(yh.shape).detach().cpu())\n",
    "        y_pred.append(yh.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "\n",
    "    input_dict = {'y_true': y_true, 'y_pred': y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)[eval_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_pred(args, model, dataset):\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # add self-loop\n",
    "    for i in range(len(dataset)):\n",
    "        dataset.graphs[i] = dataset.graphs[i].remove_self_loop(\n",
    "        ).add_self_loop()\n",
    "\n",
    "    evaluator = Evaluator(name=args.dataset)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "\n",
    "    train_loader = DataLoader(dataset[split_idx['train']], batch_size=args.batch_size,\n",
    "                              shuffle=True, num_workers=args.num_workers, collate_fn=collate_dgl)\n",
    "    valid_loader = DataLoader(dataset[split_idx['valid']], batch_size=args.batch_size,\n",
    "                              shuffle=False, num_workers=args.num_workers, collate_fn=collate_dgl)\n",
    "    test_loader = DataLoader(dataset[split_idx['test']], batch_size=args.batch_size,\n",
    "                             shuffle=False, num_workers=args.num_workers, collate_fn=collate_dgl)\n",
    "\n",
    "    logger = Logger(args.runs, mode='max')\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        print('\\nRun {}'.format(run + 1))\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # define flag\n",
    "        flag = FLAG(args.hidden_channels, loss_func, optimizer)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=args.patience, verbose=True, mode='max')\n",
    "\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            print('epoch {}'.format(epoch))\n",
    "            loss = train(model, device, train_loader, flag)\n",
    "\n",
    "            train_metric = eval(model, device, train_loader,\n",
    "                                evaluator, dataset.eval_metric)\n",
    "            valid_metric = eval(model, device, valid_loader,\n",
    "                                evaluator, dataset.eval_metric)\n",
    "            test_metric = eval(model, device, test_loader,\n",
    "                               evaluator, dataset.eval_metric)\n",
    "\n",
    "            result = [train_metric, valid_metric, test_metric]\n",
    "\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                print(\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {train_metric:.4f}, '\n",
    "                      f'Valid: {valid_metric:.4f} '\n",
    "                      f'Test: {test_metric:.4f}')\n",
    "                print()\n",
    "\n",
    "            if early_stopping(valid_metric, model):\n",
    "                break\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='train graph property prediction')\n",
    "parser.add_argument('--dataset', type=str, default='ogbg-molhiv',\n",
    "                    choices=['ogbg-molhiv'])\n",
    "parser.add_argument('--dataset_path', type=str, default='/home/ubuntu/.dgl_dataset',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--device', type=int, default=2)\n",
    "parser.add_argument('--log_steps', type=int, default=1)\n",
    "parser.add_argument('--num_layers', type=int, default=5)\n",
    "parser.add_argument('--hidden_channels', type=int, default=300)\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--num_workers', type=int, default=0,\n",
    "                    help='number of workers (default: 0)')\n",
    "parser.add_argument('--model', type=str, default='gin')\n",
    "parser.add_argument('--epochs', type=int, default=500)\n",
    "parser.add_argument('--runs', type=int, default=5)\n",
    "parser.add_argument('--patience', type=int, default=30)\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglGraphPropPredDataset(\n",
    "    name=args.dataset, root=args.dataset_path)\n",
    "\n",
    "if args.model == 'gin':\n",
    "    model = EGIN(args.hidden_channels,\n",
    "                    dataset.num_tasks, args.num_layers,\n",
    "                    args.dropout)\n",
    "elif args.model == 'gcn':\n",
    "    model = EGCN(args.hidden_channels, dataset.num_tasks,\n",
    "                    args.num_layers, args.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_graph_pred(args, model, dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
