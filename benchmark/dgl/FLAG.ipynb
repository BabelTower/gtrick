{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from ogb.nodeproppred import DglNodePropPredDataset, Evaluator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import Logger, EarlyStopping\n",
    "from dgl.nn import GraphConv, SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import flag\n",
    "from gtrick import FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout, conv_type):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if conv_type == 'gcn':\n",
    "                if i == 0:\n",
    "                    self.convs.append(GraphConv(in_channels, hidden_channels))\n",
    "                elif i == num_layers - 1:\n",
    "                    self.convs.append(GraphConv(hidden_channels, out_channels))\n",
    "                else:\n",
    "                    self.convs.append(\n",
    "                        GraphConv(hidden_channels, hidden_channels))\n",
    "            elif conv_type == 'sage':\n",
    "                if i == 0:\n",
    "                    self.convs.append(SAGEConv(in_channels, hidden_channels, 'mean'))\n",
    "                elif i == num_layers - 1:\n",
    "                    self.convs.append(SAGEConv(hidden_channels, out_channels, 'mean'))\n",
    "                else:\n",
    "                    self.convs.append(\n",
    "                        SAGEConv(hidden_channels, hidden_channels, 'mean'))\n",
    "            \n",
    "            if i != num_layers - 1:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    # add a param perturb `perturb` to pass perturb\n",
    "    def forward(self, g, x, perturb=None):\n",
    "        # add perturb to x, note that do not use x += perturb\n",
    "        if perturb is not None:\n",
    "            x = x + perturb\n",
    "        \n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(g, x)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](g, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass flag func to train\n",
    "def train(model, g, x, y, train_idx, flag):\n",
    "    y = y[train_idx].squeeze(1)\n",
    "\n",
    "    forward = lambda perturb: model(g, x, perturb)[train_idx]\n",
    "\n",
    "    loss, out = flag(model, forward, g.num_nodes(), y)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, g, x, y, split_idx, evaluator, eval_metric):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(g, x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })[eval_metric]\n",
    "    valid_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })[eval_metric]\n",
    "    test_metric = evaluator.eval({\n",
    "        'y_true': y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })[eval_metric]\n",
    "\n",
    "    return train_metric, valid_metric, test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_node_pred(args, model, dataset):\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    evaluator = Evaluator(name=args.dataset)\n",
    "\n",
    "    g, y = dataset[0]\n",
    "\n",
    "    # add reverse edges\n",
    "    srcs, dsts = g.all_edges()\n",
    "    g.add_edges(dsts, srcs)\n",
    "\n",
    "    # add self-loop\n",
    "    print(f\"Total edges before adding self-loop {g.number_of_edges()}\")\n",
    "    g = g.remove_self_loop().add_self_loop()\n",
    "    print(f\"Total edges after adding self-loop {g.number_of_edges()}\")\n",
    "\n",
    "    g, y = g.to(device), y.to(device)\n",
    "\n",
    "    if args.dataset == 'ogbn-proteins':\n",
    "        x = g.ndata['species']\n",
    "    else:\n",
    "        x = g.ndata['feat']\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']\n",
    "\n",
    "    logger = Logger(args.runs, mode='max')\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=args.patience, verbose=True, mode='max')\n",
    "\n",
    "        if dataset.task_type == 'binary classification':\n",
    "            loss_func = nn.BCEWithLogitsLoss()\n",
    "        elif dataset.task_type == 'multiclass classification':\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # define flag, params: in_feats, loss_func, optimizer\n",
    "        flag = FLAG(g.ndata['feat'].shape[1], loss_func, optimizer)\n",
    "        \n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, g, x, y, train_idx,\n",
    "                         flag)\n",
    "            result = test(model, g, x, y, split_idx,\n",
    "                          evaluator, dataset.eval_metric)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            train_acc, valid_acc, test_acc = result\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "            if early_stopping(valid_acc, model):\n",
    "                break\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Node Property Prediction Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='train node property prediction')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"ogbn-arxiv\",\n",
    "                    choices=[\"ogbn-arxiv\"])\n",
    "parser.add_argument(\"--dataset_path\", type=str, default=\"/dev/dataset\",\n",
    "                    help=\"path to dataset\")\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--log_steps', type=int, default=1)\n",
    "parser.add_argument('--model', type=str, default='gcn')\n",
    "parser.add_argument('--num_layers', type=int, default=3)\n",
    "parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--epochs', type=int, default=500)\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--patience', type=int, default=10)\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglNodePropPredDataset(\n",
    "    name=args.dataset, \n",
    "    root=args.dataset_path\n",
    "    )\n",
    "g, _ = dataset[0]\n",
    "\n",
    "num_features = g.ndata['feat'].shape[1]\n",
    "\n",
    "model = GNN(num_features, args.hidden_channels,\n",
    "                dataset.num_classes, args.num_layers,\n",
    "                args.dropout, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node_pred(args, model, dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
