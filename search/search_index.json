{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"gtrick is an easy-to-use Python package collecting tricks for Graph Neural Networks. It tests and provides powerful tricks to boost your models' performance. Trick is all you need! ( \u4e2d\u6587\u7b80\u4ecb ) Library Highlights \u00b6 Easy-to-use : All it takes is to add a few lines of code to apply a powerful trick, with as little changes of existing code as possible. Verified Trick : All tricks implemented in gtrick are tested on our selected datasets. Only the tricks indeed improving model's performance can be collected by gtrick. Backend Free : We provide all tricks both in DGL and PyG . Whatever graph learning library you use, feel free to try it. Installation \u00b6 Warning This is a developmental release. You can install gtrick by pip: pip install gtrick Usage Example \u00b6 It is very easy to get start with gtrick. Suppose you have a model for node classification: from torch_geometric.nn import GCNConv from torch_geometric.datasets import TUDataset dataset = TUDataset ( name = 'ENZYMES' ) data = dataset [ 0 ] model = GCNConv ( dataset . num_node_features , dataset . num_classes ) out = model ( data . x , data . edge_index ) You can enhance your GNN model with only a few lines of code: from torch_geometric.nn import GCNConv from torch_geometric.datasets import TUDataset from gtrick import random_feature dataset = TUDataset ( name = 'ENZYMES' ) data = dataset [ 0 ] model = GCNConv ( dataset . num_node_features , dataset . num_classes ) h = random_feature ( data . x ) out = model ( h , data . edge_index ) Implemented Tricks \u00b6 Trick Example Task Reference VirtualNode DGL PyG graph OGB Graph Property Prediction Examples FLAG DGL PyG node* graph Robust Optimization as Data Augmentation for Large-scale Graphs Fingerprint DGL PyG molecular graph* Extended-Connectivity Fingerprints Random Feature DGL PyG graph* Random Features Strengthen Graph Neural Networks Label Propagation DGL PyG node* Learning from Labeled and Unlabeled Datawith Label Propagation Correct & Smooth DGL PyG node* Combining Label Propagation And Simple Models Out-performs Graph Neural Networks Common Neighbors DGL PyG link* Link Prediction with Structural Information Resource Allocation DGL PyG link* Link Prediction with Structural Information Adamic Adar DGL PyG link* Link Prediction with Structural Information Anchor Distance DGL PyG link* Link Prediction with Structural Information","title":"Home"},{"location":"#library-highlights","text":"Easy-to-use : All it takes is to add a few lines of code to apply a powerful trick, with as little changes of existing code as possible. Verified Trick : All tricks implemented in gtrick are tested on our selected datasets. Only the tricks indeed improving model's performance can be collected by gtrick. Backend Free : We provide all tricks both in DGL and PyG . Whatever graph learning library you use, feel free to try it.","title":"Library Highlights"},{"location":"#installation","text":"Warning This is a developmental release. You can install gtrick by pip: pip install gtrick","title":"Installation"},{"location":"#usage-example","text":"It is very easy to get start with gtrick. Suppose you have a model for node classification: from torch_geometric.nn import GCNConv from torch_geometric.datasets import TUDataset dataset = TUDataset ( name = 'ENZYMES' ) data = dataset [ 0 ] model = GCNConv ( dataset . num_node_features , dataset . num_classes ) out = model ( data . x , data . edge_index ) You can enhance your GNN model with only a few lines of code: from torch_geometric.nn import GCNConv from torch_geometric.datasets import TUDataset from gtrick import random_feature dataset = TUDataset ( name = 'ENZYMES' ) data = dataset [ 0 ] model = GCNConv ( dataset . num_node_features , dataset . num_classes ) h = random_feature ( data . x ) out = model ( h , data . edge_index )","title":"Usage Example"},{"location":"#implemented-tricks","text":"Trick Example Task Reference VirtualNode DGL PyG graph OGB Graph Property Prediction Examples FLAG DGL PyG node* graph Robust Optimization as Data Augmentation for Large-scale Graphs Fingerprint DGL PyG molecular graph* Extended-Connectivity Fingerprints Random Feature DGL PyG graph* Random Features Strengthen Graph Neural Networks Label Propagation DGL PyG node* Learning from Labeled and Unlabeled Datawith Label Propagation Correct & Smooth DGL PyG node* Combining Label Propagation And Simple Models Out-performs Graph Neural Networks Common Neighbors DGL PyG link* Link Prediction with Structural Information Resource Allocation DGL PyG link* Link Prediction with Structural Information Adamic Adar DGL PyG link* Link Prediction with Structural Information Anchor Distance DGL PyG link* Link Prediction with Structural Information","title":"Implemented Tricks"},{"location":"benchmark/dgl/","text":"DGL Benchmark \u00b6 Graph Property Prediction \u00b6 ogbg-molhiv \u00b6 Trick GCN GIN \u2014 0.7683 \u00b1 0.0107 0.7708 \u00b1 0.0138 +Virtual Node 0.7330 \u00b1 0.0293 0.7673 \u00b1 0.0082 +FLAG 0.7588 \u00b1 0.0098 0.7652 \u00b1 0.0161 +Random Feature 0.7721 \u00b1 0.0143 0.7655 \u00b1 0.0092 Random Forest + Fingerprint 0.8218 \u00b1 0.0022 -- ogbg-ppa \u00b6 Trick GCN GIN \u2014 0.6664 \u00b1 0.0097 0.6849 \u00b1 0.0308 +Virtual Node 0.6695 \u00b1 0.0013 0.7090 \u00b1 0.0187 Node Property Prediction \u00b6 ogbn-arxiv \u00b6 Trick GCN SAGE \u2014 0.7175 \u00b1 0.0026 0.7165 \u00b1 0.0026 +FLAG 0.7201 \u00b1 0.0016 0.7189 \u00b1 0.0017 +Label Propagation 0.7194 \u00b1 0.0013 0.7188 \u00b1 0.0021 +Correct & Smooth 0.7274 \u00b1 0.0039 0.7347 \u00b1 0.0017 Link Property Prediction \u00b6 ogbn-collab \u00b6 Trick GCN SAGE \u2014 0.4718 \u00b1 0.0093 0.5140 \u00b1 0.0040 +Common Neighbors 0.5332 \u00b1 0.0019 0.5370 \u00b1 0.0034 +Resource Allocation 0.5024 \u00b1 0.0092 0.4787 \u00b1 0.0060 +Adamic Adar 0.5283 \u00b1 0.0048 0.5291 \u00b1 0.0032 +AnchorDistance 0.4740 \u00b1 0.0135 0.4290 \u00b1 0.0107","title":"DGL"},{"location":"benchmark/dgl/#dgl-benchmark","text":"","title":"DGL Benchmark"},{"location":"benchmark/dgl/#graph-property-prediction","text":"","title":"Graph Property Prediction"},{"location":"benchmark/dgl/#ogbg-molhiv","text":"Trick GCN GIN \u2014 0.7683 \u00b1 0.0107 0.7708 \u00b1 0.0138 +Virtual Node 0.7330 \u00b1 0.0293 0.7673 \u00b1 0.0082 +FLAG 0.7588 \u00b1 0.0098 0.7652 \u00b1 0.0161 +Random Feature 0.7721 \u00b1 0.0143 0.7655 \u00b1 0.0092 Random Forest + Fingerprint 0.8218 \u00b1 0.0022 --","title":"ogbg-molhiv"},{"location":"benchmark/dgl/#ogbg-ppa","text":"Trick GCN GIN \u2014 0.6664 \u00b1 0.0097 0.6849 \u00b1 0.0308 +Virtual Node 0.6695 \u00b1 0.0013 0.7090 \u00b1 0.0187","title":"ogbg-ppa"},{"location":"benchmark/dgl/#node-property-prediction","text":"","title":"Node Property Prediction"},{"location":"benchmark/dgl/#ogbn-arxiv","text":"Trick GCN SAGE \u2014 0.7175 \u00b1 0.0026 0.7165 \u00b1 0.0026 +FLAG 0.7201 \u00b1 0.0016 0.7189 \u00b1 0.0017 +Label Propagation 0.7194 \u00b1 0.0013 0.7188 \u00b1 0.0021 +Correct & Smooth 0.7274 \u00b1 0.0039 0.7347 \u00b1 0.0017","title":"ogbn-arxiv"},{"location":"benchmark/dgl/#link-property-prediction","text":"","title":"Link Property Prediction"},{"location":"benchmark/dgl/#ogbn-collab","text":"Trick GCN SAGE \u2014 0.4718 \u00b1 0.0093 0.5140 \u00b1 0.0040 +Common Neighbors 0.5332 \u00b1 0.0019 0.5370 \u00b1 0.0034 +Resource Allocation 0.5024 \u00b1 0.0092 0.4787 \u00b1 0.0060 +Adamic Adar 0.5283 \u00b1 0.0048 0.5291 \u00b1 0.0032 +AnchorDistance 0.4740 \u00b1 0.0135 0.4290 \u00b1 0.0107","title":"ogbn-collab"},{"location":"benchmark/pyg/","text":"PyG Benchmark \u00b6 Graph Property Prediction \u00b6 ogbg-molhiv \u00b6 Trick GCN GIN \u2014 0.7690 \u00b1 0.0053 0.7778 \u00b1 0.0130 +Virtual Node 0.7581 \u00b1 0.0135 0.7713 \u00b1 0.0036 +FLAG 0.7627 \u00b1 0.0124 0.7764 \u00b1 0.0083 +Random Feature 0.7743 \u00b1 0.0134 0.7692 \u00b1 0.0065 Random Forest + Fingerprint 0.8218 \u00b1 0.0022 -- ogbg-ppa \u00b6 Trick GCN GIN \u2014 0.6787 \u00b1 0.0091 0.6833 \u00b1 0.0087 +Virtual Node 0.6747 \u00b1 0.0060 0.6901 \u00b1 0.0277 Node Property Prediction \u00b6 ogbn-arxiv \u00b6 Trick GCN SAGE \u2014 0.7167 \u00b1 0.0022 0.7167 \u00b1 0.0025 +FLAG 0.7187 \u00b1 0.0020 0.7206 \u00b1 0.0013 +Label Propagation 0.7212 \u00b1 0.0006 0.7197 \u00b1 0.0020 +Correct & Smooth 0.7220 \u00b1 0.0037 0.7264 \u00b1 0.0004 Link Property Prediction \u00b6 ogbn-collab \u00b6 Trick GCN SAGE \u2014 0.4763 \u00b1 0.0279 0.5171 \u00b1 0.0100 +Common Neighbors 0.5422 \u00b1 0.0023 0.5368 \u00b1 0.0020 +Resource Allocation 0.5033 \u00b1 0.0142 0.5093 \u00b1 0.0055 +Adamic Adar 0.5345 \u00b1 0.0023 0.5376 \u00b1 0.0010 +AnchorDistance 0.4605 \u00b1 0.0120 0.4553 \u00b1 0.0237","title":"PyG"},{"location":"benchmark/pyg/#pyg-benchmark","text":"","title":"PyG Benchmark"},{"location":"benchmark/pyg/#graph-property-prediction","text":"","title":"Graph Property Prediction"},{"location":"benchmark/pyg/#ogbg-molhiv","text":"Trick GCN GIN \u2014 0.7690 \u00b1 0.0053 0.7778 \u00b1 0.0130 +Virtual Node 0.7581 \u00b1 0.0135 0.7713 \u00b1 0.0036 +FLAG 0.7627 \u00b1 0.0124 0.7764 \u00b1 0.0083 +Random Feature 0.7743 \u00b1 0.0134 0.7692 \u00b1 0.0065 Random Forest + Fingerprint 0.8218 \u00b1 0.0022 --","title":"ogbg-molhiv"},{"location":"benchmark/pyg/#ogbg-ppa","text":"Trick GCN GIN \u2014 0.6787 \u00b1 0.0091 0.6833 \u00b1 0.0087 +Virtual Node 0.6747 \u00b1 0.0060 0.6901 \u00b1 0.0277","title":"ogbg-ppa"},{"location":"benchmark/pyg/#node-property-prediction","text":"","title":"Node Property Prediction"},{"location":"benchmark/pyg/#ogbn-arxiv","text":"Trick GCN SAGE \u2014 0.7167 \u00b1 0.0022 0.7167 \u00b1 0.0025 +FLAG 0.7187 \u00b1 0.0020 0.7206 \u00b1 0.0013 +Label Propagation 0.7212 \u00b1 0.0006 0.7197 \u00b1 0.0020 +Correct & Smooth 0.7220 \u00b1 0.0037 0.7264 \u00b1 0.0004","title":"ogbn-arxiv"},{"location":"benchmark/pyg/#link-property-prediction","text":"","title":"Link Property Prediction"},{"location":"benchmark/pyg/#ogbn-collab","text":"Trick GCN SAGE \u2014 0.4763 \u00b1 0.0279 0.5171 \u00b1 0.0100 +Common Neighbors 0.5422 \u00b1 0.0023 0.5368 \u00b1 0.0020 +Resource Allocation 0.5033 \u00b1 0.0142 0.5093 \u00b1 0.0055 +Adamic Adar 0.5345 \u00b1 0.0023 0.5376 \u00b1 0.0010 +AnchorDistance 0.4605 \u00b1 0.0120 0.4553 \u00b1 0.0237","title":"ogbn-collab"},{"location":"dgl/correct_and_smooth/","text":"gtrick . dgl . CorrectAndSmooth ( num_correction_layers , correction_alpha , num_smoothing_layers , smoothing_alpha , autoscale = True , scale = 1.0 ) \u00b6 Bases: nn . Module The correct and smooth (C&S) post-processing model from the \"Combining Label Propagation And Simple Models Out-performs Graph Neural Networks paper, where soft predictions \\(\\mathbf{Z}\\) (obtained from a simple base predictor) are first corrected based on ground-truth training label information \\(\\mathbf{Y}\\) and residual propagation \\[ \\mathbf{e}^{(0)}_i = \\begin{cases} \\mathbf{y}_i - \\mathbf{z}_i, \\text{if }i \\text{ is training node,}\\\\ \\mathbf{0}, \\text{else} \\end{cases} \\] \\[ \\mathbf{E}^{(\\ell)} = \\alpha_1 \\mathbf{D}^{-1/2}\\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{E}^{(\\ell - 1)} + (1 - \\alpha_1) \\mathbf{E}^{(\\ell - 1)} \\] \\[ \\mathbf{\\hat{Z}} = \\mathbf{Z} + \\gamma \\cdot \\mathbf{E}^{(L_1)} \\] where \\(\\gamma\\) denotes the scaling factor (either fixed or automatically determined), and then smoothed over the graph via label propagation \\[ \\mathbf{\\hat{z}}^{(0)}_i = \\begin{cases} \\mathbf{y}_i, \\text{if }i\\text{ is training node,}\\\\ \\mathbf{\\hat{z}}_i, \\text{else} \\end{cases} \\] \\[ \\mathbf{\\hat{Z}}^{(\\ell)} = \\alpha_2 \\mathbf{D}^{-1/2}\\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{\\hat{Z}}^{(\\ell - 1)} + (1 - \\alpha_1) \\mathbf{\\hat{Z}}^{(\\ell - 1)} \\] to obtain the final prediction \\(\\mathbf{\\hat{Z}}^{(L_2)}\\) . This trick is helpful for Node Level Task . Note To use this trick, call correct at first, then call smooth . Examples: CorrectAndSmooth (DGL) Parameters: Name Type Description Default num_correction_layers int The number of propagations \\(L_1\\) . required correction_alpha float The \\(\\alpha_1\\) coefficient. required num_smoothing_layers int The number of propagations \\(L_2\\) . required smoothing_alpha float The \\(\\alpha_2\\) coefficient. required autoscale bool If set to True , will automatically determine the scaling factor \\(\\gamma\\) . True scale float The scaling factor \\(\\gamma\\) , in case autoscale = False . 1.0 correct ( graph , y_soft , y_true , mask , edge_weight = None ) \u00b6 Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y_soft Tensor The soft predictions \\(\\mathbf{Z}\\) obtained from a simple base predictor. required y_true Tensor The ground-truth label information \\(\\mathbf{Y}\\) of training nodes. required mask LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. required edge_weight Tensor The edge weights. None Returns: Type Description torch . Tensor The corrected prediction. smooth ( graph , y_soft , y_true , mask , edge_weight = None ) \u00b6 Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y_soft Tensor The soft predictions \\(\\mathbf{Z}\\) obtained from a simple base predictor. required y_true Tensor The ground-truth label information \\(\\mathbf{Y}\\) of training nodes. required mask LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. required edge_weight Tensor The edge weights. None Returns: Type Description torch . Tensor The final prediction.","title":"CorrectAndSmooth"},{"location":"dgl/correct_and_smooth/#gtrick.dgl.CorrectAndSmooth","text":"Bases: nn . Module The correct and smooth (C&S) post-processing model from the \"Combining Label Propagation And Simple Models Out-performs Graph Neural Networks paper, where soft predictions \\(\\mathbf{Z}\\) (obtained from a simple base predictor) are first corrected based on ground-truth training label information \\(\\mathbf{Y}\\) and residual propagation \\[ \\mathbf{e}^{(0)}_i = \\begin{cases} \\mathbf{y}_i - \\mathbf{z}_i, \\text{if }i \\text{ is training node,}\\\\ \\mathbf{0}, \\text{else} \\end{cases} \\] \\[ \\mathbf{E}^{(\\ell)} = \\alpha_1 \\mathbf{D}^{-1/2}\\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{E}^{(\\ell - 1)} + (1 - \\alpha_1) \\mathbf{E}^{(\\ell - 1)} \\] \\[ \\mathbf{\\hat{Z}} = \\mathbf{Z} + \\gamma \\cdot \\mathbf{E}^{(L_1)} \\] where \\(\\gamma\\) denotes the scaling factor (either fixed or automatically determined), and then smoothed over the graph via label propagation \\[ \\mathbf{\\hat{z}}^{(0)}_i = \\begin{cases} \\mathbf{y}_i, \\text{if }i\\text{ is training node,}\\\\ \\mathbf{\\hat{z}}_i, \\text{else} \\end{cases} \\] \\[ \\mathbf{\\hat{Z}}^{(\\ell)} = \\alpha_2 \\mathbf{D}^{-1/2}\\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{\\hat{Z}}^{(\\ell - 1)} + (1 - \\alpha_1) \\mathbf{\\hat{Z}}^{(\\ell - 1)} \\] to obtain the final prediction \\(\\mathbf{\\hat{Z}}^{(L_2)}\\) . This trick is helpful for Node Level Task . Note To use this trick, call correct at first, then call smooth . Examples: CorrectAndSmooth (DGL) Parameters: Name Type Description Default num_correction_layers int The number of propagations \\(L_1\\) . required correction_alpha float The \\(\\alpha_1\\) coefficient. required num_smoothing_layers int The number of propagations \\(L_2\\) . required smoothing_alpha float The \\(\\alpha_2\\) coefficient. required autoscale bool If set to True , will automatically determine the scaling factor \\(\\gamma\\) . True scale float The scaling factor \\(\\gamma\\) , in case autoscale = False . 1.0","title":"CorrectAndSmooth"},{"location":"dgl/correct_and_smooth/#gtrick.dgl.correct_and_smooth.CorrectAndSmooth.correct","text":"Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y_soft Tensor The soft predictions \\(\\mathbf{Z}\\) obtained from a simple base predictor. required y_true Tensor The ground-truth label information \\(\\mathbf{Y}\\) of training nodes. required mask LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. required edge_weight Tensor The edge weights. None Returns: Type Description torch . Tensor The corrected prediction.","title":"correct()"},{"location":"dgl/correct_and_smooth/#gtrick.dgl.correct_and_smooth.CorrectAndSmooth.smooth","text":"Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y_soft Tensor The soft predictions \\(\\mathbf{Z}\\) obtained from a simple base predictor. required y_true Tensor The ground-truth label information \\(\\mathbf{Y}\\) of training nodes. required mask LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. required edge_weight Tensor The edge weights. None Returns: Type Description torch . Tensor The final prediction.","title":"smooth()"},{"location":"dgl/edge_feat/","text":"gtrick . dgl . CommonNeighbors ( adj , edge_attr = None , batch_size = 64 ) \u00b6 Compute the common neighbors of two nodes in a graph. Example EdgeFeat (dgl) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated common neighbors feature. gtrick . dgl . ResourceAllocation ( adj , edge_attr = None , batch_size = 64 ) \u00b6 Bases: object Compute the resource allocation of two nodes in a graph. Resource allocation of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{|\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . Example EdgeFeat (PyG) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated resource allocation feature. gtrick . dgl . AdamicAdar ( adj , edge_attr = None , batch_size = 64 ) \u00b6 Bases: object Computes the adamic adar of two nodes in a graph. Adamic-Adar index of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . This index leads to zero-division for nodes only connected via self-loops. It is intended to be used when no self-loops are present. Example EdgeFeat (PyG) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges = None ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). None Returns: Type Description torch . Tensor The calculated adamic adar feature. gtrick . dgl . AnchorDistance ( g , num_samples , k , ka , max_spd = 5 ) \u00b6 Bases: object Computes the anchor distance of two nodes in a graph. The anchor distance randomly selects \\(k_a\\) nodes from \\(V\\) to be anchor nodes and then calculates the shortest path starting from these anchor nodes to any other nodes. After that, the distance between \\(u\\) and \\(v\\) can be estimated by: \\[ d_{u, v}=\\frac{1}{K_{A}} \\sum_{i=1}^{K_{A}} d_{u, a_{i}}+d_{v, a_{i}} \\] To reduce the randomness, it uses \\(k\\) anchor sets to generate multiple distance features. Example EdgeFeat (PyG) Parameters: Name Type Description Default g dgl . DGLGraph Graph data. required num_samples int The number of times to sample anchor sets. required k int The size of sampled anchor sets. required ka int The number of anchor nodes. required max_spd int The max shortest distance. 5 __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated anchor distance feature.","title":"Edge Feature"},{"location":"dgl/edge_feat/#gtrick.dgl.CommonNeighbors","text":"Compute the common neighbors of two nodes in a graph. Example EdgeFeat (dgl) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"CommonNeighbors"},{"location":"dgl/edge_feat/#gtrick.dgl.edge_feat.CommonNeighbors.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated common neighbors feature.","title":"__call__()"},{"location":"dgl/edge_feat/#gtrick.dgl.ResourceAllocation","text":"Bases: object Compute the resource allocation of two nodes in a graph. Resource allocation of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{|\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . Example EdgeFeat (PyG) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"ResourceAllocation"},{"location":"dgl/edge_feat/#gtrick.dgl.edge_feat.ResourceAllocation.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated resource allocation feature.","title":"__call__()"},{"location":"dgl/edge_feat/#gtrick.dgl.AdamicAdar","text":"Bases: object Computes the adamic adar of two nodes in a graph. Adamic-Adar index of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . This index leads to zero-division for nodes only connected via self-loops. It is intended to be used when no self-loops are present. Example EdgeFeat (PyG) Parameters: Name Type Description Default adj SparseTensor Adjacency matrix. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"AdamicAdar"},{"location":"dgl/edge_feat/#gtrick.dgl.edge_feat.AdamicAdar.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). None Returns: Type Description torch . Tensor The calculated adamic adar feature.","title":"__call__()"},{"location":"dgl/edge_feat/#gtrick.dgl.AnchorDistance","text":"Bases: object Computes the anchor distance of two nodes in a graph. The anchor distance randomly selects \\(k_a\\) nodes from \\(V\\) to be anchor nodes and then calculates the shortest path starting from these anchor nodes to any other nodes. After that, the distance between \\(u\\) and \\(v\\) can be estimated by: \\[ d_{u, v}=\\frac{1}{K_{A}} \\sum_{i=1}^{K_{A}} d_{u, a_{i}}+d_{v, a_{i}} \\] To reduce the randomness, it uses \\(k\\) anchor sets to generate multiple distance features. Example EdgeFeat (PyG) Parameters: Name Type Description Default g dgl . DGLGraph Graph data. required num_samples int The number of times to sample anchor sets. required k int The size of sampled anchor sets. required ka int The number of anchor nodes. required max_spd int The max shortest distance. 5","title":"AnchorDistance"},{"location":"dgl/edge_feat/#gtrick.dgl.edge_feat.AnchorDistance.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated anchor distance feature.","title":"__call__()"},{"location":"dgl/label_prop/","text":"gtrick . dgl . LabelPropagation ( num_layers , alpha ) \u00b6 Bases: nn . Module The label propagation operator from the \"Learning from Labeled and Unlabeled Datawith Label Propagation\" paper. This trick is helpful for Node Level Task . \\[ \\mathbf{Y}^{\\prime} = \\alpha \\cdot \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{Y} + (1 - \\\\alpha) \\mathbf{Y}, \\] where unlabeled data is inferred by labeled data via propagation. Examples: LabelPropagation (DGL) Parameters: Name Type Description Default num_layers int The number of propagations. required alpha float The \\(\\alpha\\) coefficient. required forward ( graph , y , mask = None , edge_weight = None , post_step = lambda y : y . clamp_ ( 0.0 , 1.0 )) \u00b6 Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y torch . Tensor The ground-truth label information of training nodes. required mask torch.LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. None edge_weight torch . Tensor The edge weights. None post_step Callable [[ torch . Tensor ], torch . Tensor ] The post-process function. lambda y: y.clamp_(0.0, 1.0) Returns: Type Description torch . Tensor The obtained prediction.","title":"LabelPropagation"},{"location":"dgl/label_prop/#gtrick.dgl.LabelPropagation","text":"Bases: nn . Module The label propagation operator from the \"Learning from Labeled and Unlabeled Datawith Label Propagation\" paper. This trick is helpful for Node Level Task . \\[ \\mathbf{Y}^{\\prime} = \\alpha \\cdot \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2} \\mathbf{Y} + (1 - \\\\alpha) \\mathbf{Y}, \\] where unlabeled data is inferred by labeled data via propagation. Examples: LabelPropagation (DGL) Parameters: Name Type Description Default num_layers int The number of propagations. required alpha float The \\(\\alpha\\) coefficient. required","title":"LabelPropagation"},{"location":"dgl/label_prop/#gtrick.dgl.label_prop.LabelPropagation.forward","text":"Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required y torch . Tensor The ground-truth label information of training nodes. required mask torch.LongTensor or BoolTensor A mask or index tensor denoting which nodes were used for training. None edge_weight torch . Tensor The edge weights. None post_step Callable [[ torch . Tensor ], torch . Tensor ] The post-process function. lambda y: y.clamp_(0.0, 1.0) Returns: Type Description torch . Tensor The obtained prediction.","title":"forward()"},{"location":"dgl/virtual_node/","text":"gtrick . dgl . VirtualNode ( in_feats , out_feats , dropout = 0.5 , residual = False ) \u00b6 Bases: nn . Module Virtual Node from OGB Graph Property Prediction Examples . It adds an virtual node to all nodes in the graph. This trick is helpful for Graph Level Task . Note To use this trick, call update_node_emb at first, then call update_vn_emb . Examples: VirtualNode (DGL) Parameters: Name Type Description Default in_feats int Feature size before conv layer. required out_feats int Feature size after conv layer. required dropout float Dropout rate on virtual node embedding. Default: 0.5. 0.5 residual bool If True, use residual connection. Default: False. False update_node_emb ( graph , x , vx = None ) \u00b6 Add message from virtual nodes to graph nodes. Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required x torch . Tensor The input node feature. required vx torch . Tensor Optional virtual node embedding. Default: None. None Returns: Type Description torch . Tensor The output node feature. torch . Tensor The output virtual node embedding. update_vn_emb ( graph , x , vx ) \u00b6 Add message from graph nodes to virtual node. Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required x torch . Tensor The input node feature. required vx torch . Tensor Optional virtual node embedding. required Returns: Type Description torch . Tensor The output virtual node embedding.","title":"VirtualNode"},{"location":"dgl/virtual_node/#gtrick.dgl.VirtualNode","text":"Bases: nn . Module Virtual Node from OGB Graph Property Prediction Examples . It adds an virtual node to all nodes in the graph. This trick is helpful for Graph Level Task . Note To use this trick, call update_node_emb at first, then call update_vn_emb . Examples: VirtualNode (DGL) Parameters: Name Type Description Default in_feats int Feature size before conv layer. required out_feats int Feature size after conv layer. required dropout float Dropout rate on virtual node embedding. Default: 0.5. 0.5 residual bool If True, use residual connection. Default: False. False","title":"VirtualNode"},{"location":"dgl/virtual_node/#gtrick.dgl.virtual_node.VirtualNode.update_node_emb","text":"Add message from virtual nodes to graph nodes. Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required x torch . Tensor The input node feature. required vx torch . Tensor Optional virtual node embedding. Default: None. None Returns: Type Description torch . Tensor The output node feature. torch . Tensor The output virtual node embedding.","title":"update_node_emb()"},{"location":"dgl/virtual_node/#gtrick.dgl.virtual_node.VirtualNode.update_vn_emb","text":"Add message from graph nodes to virtual node. Parameters: Name Type Description Default graph dgl . DGLGraph The graph. required x torch . Tensor The input node feature. required vx torch . Tensor Optional virtual node embedding. required Returns: Type Description torch . Tensor The output virtual node embedding.","title":"update_vn_emb()"},{"location":"pyg/correct_and_smooth/","text":"gtrick.pyg.CorrectAndSmooth \u00b6 Note CorrectAndSmooth has been implemented in PyG. You can use it by from torch_geometric.nn import CorrectAndSmooth or use gtrick to manage all tricks. Since the two is the same, you can refer to the PyG document: torch_geometric.nn.CorrectAndSmooth . Examples: CorrectAndSmooth (PyG)","title":"CorrectAndSmooth"},{"location":"pyg/correct_and_smooth/#gtrickpygcorrectandsmooth","text":"Note CorrectAndSmooth has been implemented in PyG. You can use it by from torch_geometric.nn import CorrectAndSmooth or use gtrick to manage all tricks. Since the two is the same, you can refer to the PyG document: torch_geometric.nn.CorrectAndSmooth . Examples: CorrectAndSmooth (PyG)","title":"gtrick.pyg.CorrectAndSmooth"},{"location":"pyg/edge_feat/","text":"gtrick . pyg . CommonNeighbors ( edge_index , edge_attr = None , batch_size = 64 ) \u00b6 Bases: object Compute the common neighbors of two nodes in a graph. Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated common neighbors feature. gtrick . pyg . ResourceAllocation ( edge_index , edge_attr = None , batch_size = 64 ) \u00b6 Bases: object Compute the resource allocation of two nodes in a graph. Resource allocation of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{|\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated resource allocation feature. gtrick . pyg . AdamicAdar ( edge_index , edge_attr = None , batch_size = 64 ) \u00b6 Bases: object Computes the adamic adar of two nodes in a graph. Adamic-Adar index of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . This index leads to zero-division for nodes only connected via self-loops. It is intended to be used when no self-loops are present. Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64 __call__ ( edges = None ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). None Returns: Type Description torch . Tensor The calculated adamic adar feature. gtrick . pyg . AnchorDistance ( data , num_samples , k , ka , max_spd = 5 , to_undirected = True ) \u00b6 Bases: object Computes the anchor distance of two nodes in a graph. The anchor distance randomly selects \\(k_a\\) nodes from \\(V\\) to be anchor nodes and then calculates the shortest path starting from these anchor nodes to any other nodes. After that, the distance between \\(u\\) and \\(v\\) can be estimated by: \\[ d_{u, v}=\\frac{1}{K_{A}} \\sum_{i=1}^{K_{A}} d_{u, a_{i}}+d_{v, a_{i}} \\] To reduce the randomness, it uses \\(k\\) anchor sets to generate multiple distance features. Example EdgeFeat (PyG) Parameters: Name Type Description Default data torch_geometric . data . Data Graph data. required num_samples int The number of times to sample anchor sets. required k int The size of sampled anchor sets. required ka int The number of anchor nodes. required max_spd int The max shortest distance. 5 to_undirected bool Converts the graph to an undirected graph. True __call__ ( edges ) \u00b6 Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated anchor distance feature.","title":"Edge Feature"},{"location":"pyg/edge_feat/#gtrick.pyg.CommonNeighbors","text":"Bases: object Compute the common neighbors of two nodes in a graph. Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"CommonNeighbors"},{"location":"pyg/edge_feat/#gtrick.pyg.edge_feat.CommonNeighbors.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated common neighbors feature.","title":"__call__()"},{"location":"pyg/edge_feat/#gtrick.pyg.ResourceAllocation","text":"Bases: object Compute the resource allocation of two nodes in a graph. Resource allocation of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{|\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"ResourceAllocation"},{"location":"pyg/edge_feat/#gtrick.pyg.edge_feat.ResourceAllocation.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated resource allocation feature.","title":"__call__()"},{"location":"pyg/edge_feat/#gtrick.pyg.AdamicAdar","text":"Bases: object Computes the adamic adar of two nodes in a graph. Adamic-Adar index of \\(u\\) and \\(v\\) is defined as \\[ \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|} \\] where \\(\\Gamma(u)\\) denotes the set of neighbors of \\(u\\) . This index leads to zero-division for nodes only connected via self-loops. It is intended to be used when no self-loops are present. Example EdgeFeat (PyG) Parameters: Name Type Description Default edge_index torch . LongTensor Graph connectivity. required edge_attr torch . Tensor Edge feature. None batch_size int The batch size to compute common neighbors. 64","title":"AdamicAdar"},{"location":"pyg/edge_feat/#gtrick.pyg.edge_feat.AdamicAdar.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). None Returns: Type Description torch . Tensor The calculated adamic adar feature.","title":"__call__()"},{"location":"pyg/edge_feat/#gtrick.pyg.AnchorDistance","text":"Bases: object Computes the anchor distance of two nodes in a graph. The anchor distance randomly selects \\(k_a\\) nodes from \\(V\\) to be anchor nodes and then calculates the shortest path starting from these anchor nodes to any other nodes. After that, the distance between \\(u\\) and \\(v\\) can be estimated by: \\[ d_{u, v}=\\frac{1}{K_{A}} \\sum_{i=1}^{K_{A}} d_{u, a_{i}}+d_{v, a_{i}} \\] To reduce the randomness, it uses \\(k\\) anchor sets to generate multiple distance features. Example EdgeFeat (PyG) Parameters: Name Type Description Default data torch_geometric . data . Data Graph data. required num_samples int The number of times to sample anchor sets. required k int The size of sampled anchor sets. required ka int The number of anchor nodes. required max_spd int The max shortest distance. 5 to_undirected bool Converts the graph to an undirected graph. True","title":"AnchorDistance"},{"location":"pyg/edge_feat/#gtrick.pyg.edge_feat.AnchorDistance.__call__","text":"Parameters: Name Type Description Default edges torch . Tensor The edges with the shape (num_edges, 2). required Returns: Type Description torch . Tensor The calculated anchor distance feature.","title":"__call__()"},{"location":"pyg/fingerprint/","text":"gtrick . ogb2fp ( name , root = 'dataset' , fp_type = [ 'morgan' , 'maccs' ]) \u00b6 Generate fingerprint features for OGB datasets. Molecular fingerprints are a way to represent molecules as mathematical objects. Note To use this trick, you should install rdkit at first: pip install rdkit Example Fingerprint (DGL) , Fingerprint (PyG) Parameters: Name Type Description Default name str Name of the dataset. required root str Root directory to store the dataset folder. 'dataset' fp_type list of str The types of generated fingerprint. Can be the following values: morgan : Morgan fingerprint. maccs : MACCS keys. rdkit : RDKit topological fingerprint. ['morgan', 'maccs'] Returns: Type Description torch . Tensor The generated fingerprint features. torch . Tensor The ground truth label. gtrick . smiles2fp ( smiles , fp_type ) \u00b6 Convert smile strings to fingerprint. Molecular fingerprints are a way to represent molecules as mathematical objects. Note To use this trick, you should install rdkit at first: pip install rdkit Parameters: Name Type Description Default smiles list of str The smile strings to convert. required fp_type list of str The types of generated fingerprint. Can be the following values: morgan : Morgan fingerprint. maccs : MACCS keys. rdkit : RDKit topological fingerprint. required Returns: Type Description np . array The generated fingerprint features.","title":"Fingerprint"},{"location":"pyg/fingerprint/#gtrick.ogb2fp","text":"Generate fingerprint features for OGB datasets. Molecular fingerprints are a way to represent molecules as mathematical objects. Note To use this trick, you should install rdkit at first: pip install rdkit Example Fingerprint (DGL) , Fingerprint (PyG) Parameters: Name Type Description Default name str Name of the dataset. required root str Root directory to store the dataset folder. 'dataset' fp_type list of str The types of generated fingerprint. Can be the following values: morgan : Morgan fingerprint. maccs : MACCS keys. rdkit : RDKit topological fingerprint. ['morgan', 'maccs'] Returns: Type Description torch . Tensor The generated fingerprint features. torch . Tensor The ground truth label.","title":"ogb2fp()"},{"location":"pyg/fingerprint/#gtrick.smiles2fp","text":"Convert smile strings to fingerprint. Molecular fingerprints are a way to represent molecules as mathematical objects. Note To use this trick, you should install rdkit at first: pip install rdkit Parameters: Name Type Description Default smiles list of str The smile strings to convert. required fp_type list of str The types of generated fingerprint. Can be the following values: morgan : Morgan fingerprint. maccs : MACCS keys. rdkit : RDKit topological fingerprint. required Returns: Type Description np . array The generated fingerprint features.","title":"smiles2fp()"},{"location":"pyg/flag/","text":"gtrick . FLAG ( emb_dim , loss_func , optimizer , m = 3 , step_size = 0.001 , mag =- 1 ) \u00b6 FLAG is an adversarial data augmentation method for Graph Neural Networks, which comes from Robust Optimization as Data Augmentation for Large-scale Graphs . This trick is helpful for Node Level Task and Graph Level Task . Example FLAG (DGL) , FALG (PyG) Parameters: Name Type Description Default emb_dim int Node feature dim. required loss_func torch . nn . Module Loss function. required optimizer torch.optim.Optimizer) Optimizer. required m int Ascent steps. Train the same minibatch m times. 3 step_size float Ascent step size. If mag <= 0, perturb is initialized from uniform distribution [-step_size, step_size]. 0.001 mag float If mag > 0, it controls the max norm of perturb. -1 __call__ ( model , forward , num_nodes , y ) \u00b6 Parameters: Name Type Description Default model torch . nn . Module The model. required forward Callable [[ torch . Tensor ], torch . Tensor ] The function that inputs perturb and gets output. required num_nodes int The number of nodes. required y torch . Tensor The ground truth label. required Returns: Type Description torch . Tensor The loss. torch . Tensor The output of the model.","title":"FLAG"},{"location":"pyg/flag/#gtrick.FLAG","text":"FLAG is an adversarial data augmentation method for Graph Neural Networks, which comes from Robust Optimization as Data Augmentation for Large-scale Graphs . This trick is helpful for Node Level Task and Graph Level Task . Example FLAG (DGL) , FALG (PyG) Parameters: Name Type Description Default emb_dim int Node feature dim. required loss_func torch . nn . Module Loss function. required optimizer torch.optim.Optimizer) Optimizer. required m int Ascent steps. Train the same minibatch m times. 3 step_size float Ascent step size. If mag <= 0, perturb is initialized from uniform distribution [-step_size, step_size]. 0.001 mag float If mag > 0, it controls the max norm of perturb. -1","title":"FLAG"},{"location":"pyg/flag/#gtrick.flag.FLAG.__call__","text":"Parameters: Name Type Description Default model torch . nn . Module The model. required forward Callable [[ torch . Tensor ], torch . Tensor ] The function that inputs perturb and gets output. required num_nodes int The number of nodes. required y torch . Tensor The ground truth label. required Returns: Type Description torch . Tensor The loss. torch . Tensor The output of the model.","title":"__call__()"},{"location":"pyg/label_prop/","text":"gtrick.pyg.LabelPropagation \u00b6 Note LabelPropagation has been implemented in PyG. You can use it by from torch_geometric.nn import LabelPropagation or use gtrick to manage all tricks. Since the two is the same, you can refer to the PyG document: torch_geometric.nn.LabelPropagation . Examples: LabelPropagation (PyG)","title":"LabelPropagation"},{"location":"pyg/label_prop/#gtrickpyglabelpropagation","text":"Note LabelPropagation has been implemented in PyG. You can use it by from torch_geometric.nn import LabelPropagation or use gtrick to manage all tricks. Since the two is the same, you can refer to the PyG document: torch_geometric.nn.LabelPropagation . Examples: LabelPropagation (PyG)","title":"gtrick.pyg.LabelPropagation"},{"location":"pyg/random_feature/","text":"gtrick.random_feature \u00b6 random_feature ( x ) \u00b6 Random feature comes from Random Features Strengthen Graph Neural Networks . This trick adds a random feature to node features. Example RandomFeature (DGL) , RandomFeature (PyG) Parameters: Name Type Description Default x torch . Tensor The node feature. required Returns: Type Description torch . Tensor The node feature with a random feature.","title":"RandomFeature"},{"location":"pyg/random_feature/#gtrick.random_feature","text":"","title":"random_feature"},{"location":"pyg/random_feature/#gtrick.random_feature.random_feature","text":"Random feature comes from Random Features Strengthen Graph Neural Networks . This trick adds a random feature to node features. Example RandomFeature (DGL) , RandomFeature (PyG) Parameters: Name Type Description Default x torch . Tensor The node feature. required Returns: Type Description torch . Tensor The node feature with a random feature.","title":"random_feature()"},{"location":"pyg/virtual_node/","text":"gtrick . pyg . VirtualNode ( in_feats , out_feats , dropout = 0.5 , residual = False ) \u00b6 Bases: nn . Module Virtual Node from OGB Graph Property Prediction Examples . It adds an virtual node to all nodes in the graph. This trick is helpful for Graph Level Task . Note To use this trick, call update_node_emb at first, then call update_vn_emb . Examples: VirtualNode (PyG) Parameters: Name Type Description Default in_feats int Feature size before conv layer. required out_feats int Feature size after conv layer. required dropout float Dropout rate on virtual node embedding. 0.5 residual bool If True, use residual connection. False update_node_emb ( x , edge_index , batch , vx = None ) \u00b6 Add message from virtual nodes to graph nodes. Parameters: Name Type Description Default x torch . Tensor The input node feature. required edge_index torch . LongTensor Graph connectivity. required batch torch . LongTensor Batch vector, which assigns each node to a specific example. required vx torch . Tensor Optional virtual node embedding. None Returns: Type Description torch . Tensor The output node feature. torch . Tensor The output virtual node embedding. update_vn_emb ( x , batch , vx ) \u00b6 Add message from graph nodes to virtual node. Parameters: Name Type Description Default x torch . Tensor The input node feature. required batch LongTensor Batch vector, which assigns each node to a specific example. required vx torch . Tensor Optional virtual node embedding. required Returns: Type Description torch . Tensor The output virtual node embedding.","title":"VirtualNode"},{"location":"pyg/virtual_node/#gtrick.pyg.VirtualNode","text":"Bases: nn . Module Virtual Node from OGB Graph Property Prediction Examples . It adds an virtual node to all nodes in the graph. This trick is helpful for Graph Level Task . Note To use this trick, call update_node_emb at first, then call update_vn_emb . Examples: VirtualNode (PyG) Parameters: Name Type Description Default in_feats int Feature size before conv layer. required out_feats int Feature size after conv layer. required dropout float Dropout rate on virtual node embedding. 0.5 residual bool If True, use residual connection. False","title":"VirtualNode"},{"location":"pyg/virtual_node/#gtrick.pyg.virtual_node.VirtualNode.update_node_emb","text":"Add message from virtual nodes to graph nodes. Parameters: Name Type Description Default x torch . Tensor The input node feature. required edge_index torch . LongTensor Graph connectivity. required batch torch . LongTensor Batch vector, which assigns each node to a specific example. required vx torch . Tensor Optional virtual node embedding. None Returns: Type Description torch . Tensor The output node feature. torch . Tensor The output virtual node embedding.","title":"update_node_emb()"},{"location":"pyg/virtual_node/#gtrick.pyg.virtual_node.VirtualNode.update_vn_emb","text":"Add message from graph nodes to virtual node. Parameters: Name Type Description Default x torch . Tensor The input node feature. required batch LongTensor Batch vector, which assigns each node to a specific example. required vx torch . Tensor Optional virtual node embedding. required Returns: Type Description torch . Tensor The output virtual node embedding.","title":"update_vn_emb()"}]}